{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "462d88e8-730f-41b2-ac05-04e3a1d958b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pennylane-lightning[gpu]\n",
      "  Using cached pennylane_lightning-0.43.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Collecting pennylane>=0.41 (from pennylane-lightning[gpu])\n",
      "  Using cached pennylane-0.43.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning[gpu])\n",
      "  Using cached scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
      "Collecting pennylane-lightning-gpu (from pennylane-lightning[gpu])\n",
      "  Downloading pennylane_lightning_gpu-0.43.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from pennylane>=0.41->pennylane-lightning[gpu])\n",
      "  Using cached scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting networkx (from pennylane>=0.41->pennylane-lightning[gpu])\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting rustworkx>=0.14.0 (from pennylane>=0.41->pennylane-lightning[gpu])\n",
      "  Using cached rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting autograd (from pennylane>=0.41->pennylane-lightning[gpu])\n",
      "  Using cached autograd-1.8.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting appdirs (from pennylane>=0.41->pennylane-lightning[gpu])\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting autoray==0.8.0 (from pennylane>=0.41->pennylane-lightning[gpu])\n",
      "  Using cached autoray-0.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting cachetools (from pennylane>=0.41->pennylane-lightning[gpu])\n",
      "  Using cached cachetools-6.2.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from pennylane>=0.41->pennylane-lightning[gpu]) (2.31.0)\n",
      "Collecting tomlkit (from pennylane>=0.41->pennylane-lightning[gpu])\n",
      "  Using cached tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.11/site-packages (from pennylane>=0.41->pennylane-lightning[gpu]) (4.14.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from pennylane>=0.41->pennylane-lightning[gpu]) (24.0)\n",
      "Collecting diastatic-malt (from pennylane>=0.41->pennylane-lightning[gpu])\n",
      "  Using cached diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from pennylane>=0.41->pennylane-lightning[gpu]) (2.3.3)\n",
      "Collecting astunparse (from diastatic-malt->pennylane>=0.41->pennylane-lightning[gpu])\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting gast (from diastatic-malt->pennylane>=0.41->pennylane-lightning[gpu])\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting termcolor (from diastatic-malt->pennylane>=0.41->pennylane-lightning[gpu])\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse->diastatic-malt->pennylane>=0.41->pennylane-lightning[gpu]) (0.43.0)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /opt/conda/lib/python3.11/site-packages (from astunparse->diastatic-malt->pennylane>=0.41->pennylane-lightning[gpu]) (1.16.0)\n",
      "Collecting custatevec-cu12 (from pennylane-lightning-gpu->pennylane-lightning[gpu])\n",
      "  Downloading custatevec_cu12-1.10.1-py3-none-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from pennylane-lightning-gpu->pennylane-lightning[gpu])\n",
      "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusparse-cu12 (from pennylane-lightning-gpu->pennylane-lightning[gpu])\n",
      "  Downloading nvidia_cusparse_cu12-12.5.10.65-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12 (from pennylane-lightning-gpu->pennylane-lightning[gpu])\n",
      "  Downloading nvidia_cublas_cu12-12.9.1.4-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12 (from pennylane-lightning-gpu->pennylane-lightning[gpu])\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.9.79-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->pennylane>=0.41->pennylane-lightning[gpu]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->pennylane>=0.41->pennylane-lightning[gpu]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->pennylane>=0.41->pennylane-lightning[gpu]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->pennylane>=0.41->pennylane-lightning[gpu]) (2024.2.2)\n",
      "Using cached pennylane_lightning-0.43.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n",
      "Using cached pennylane-0.43.0-py3-none-any.whl (5.3 MB)\n",
      "Using cached autoray-0.8.0-py3-none-any.whl (934 kB)\n",
      "Using cached rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Using cached scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached autograd-1.8.0-py3-none-any.whl (51 kB)\n",
      "Using cached cachetools-6.2.1-py3-none-any.whl (11 kB)\n",
      "Using cached diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Downloading pennylane_lightning_gpu-0.43.0-cp311-cp311-manylinux_2_28_x86_64.whl (909 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m909.3/909.3 kB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading custatevec_cu12-1.10.1-py3-none-manylinux2014_x86_64.whl (72.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/72.3 MB\u001b[0m \u001b[31m133.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.9.1.4-py3-none-manylinux_2_27_x86_64.whl (581.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.2/581.2 MB\u001b[0m \u001b[31m155.0 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.9.79-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m155.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.10.65-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (366.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.5/366.5 MB\u001b[0m \u001b[31m172.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m139.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: custatevec-cu12, appdirs, tomlkit, termcolor, scipy-openblas32, scipy, rustworkx, nvidia-nvjitlink-cu12, nvidia-cuda-runtime-cu12, nvidia-cublas-cu12, networkx, gast, cachetools, autoray, autograd, astunparse, nvidia-cusparse-cu12, diastatic-malt, pennylane-lightning, pennylane, pennylane-lightning-gpu\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/21\u001b[0m [pennylane-lightning-gpu]ane]lightning]]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed appdirs-1.4.4 astunparse-1.6.3 autograd-1.8.0 autoray-0.8.0 cachetools-6.2.1 custatevec-cu12-1.10.1 diastatic-malt-2.15.2 gast-0.6.0 networkx-3.5 nvidia-cublas-cu12-12.9.1.4 nvidia-cuda-runtime-cu12-12.9.79 nvidia-cusparse-cu12-12.5.10.65 nvidia-nvjitlink-cu12-12.9.86 pennylane-0.43.0 pennylane-lightning-0.43.0 pennylane-lightning-gpu-0.43.0 rustworkx-0.17.1 scipy-1.16.2 scipy-openblas32-0.3.30.0.2 termcolor-3.1.0 tomlkit-0.13.3\n",
      "Collecting cuquantum-python\n",
      "  Downloading cuquantum_python-25.9.1.tar.gz (6.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cuquantum-python-cu12==25.09.1 (from cuquantum-python)\n",
      "  Downloading cuquantum_python_cu12-25.9.1-74-cp311-cp311-manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.21 in /opt/conda/lib/python3.11/site-packages (from cuquantum-python-cu12==25.09.1->cuquantum-python) (2.3.3)\n",
      "Collecting nvmath-python==0.6.0 (from cuquantum-python-cu12==25.09.1->cuquantum-python)\n",
      "  Downloading nvmath_python-0.6.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: custatevec-cu12<2,>=1.10.1 in /opt/conda/lib/python3.11/site-packages (from cuquantum-python-cu12==25.09.1->cuquantum-python) (1.10.1)\n",
      "Collecting cutensornet-cu12<3,>=2.9.1 (from cuquantum-python-cu12==25.09.1->cuquantum-python)\n",
      "  Downloading cutensornet_cu12-2.9.1-py3-none-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting cudensitymat-cu12<0.4,>=0.3.1 (from cuquantum-python-cu12==25.09.1->cuquantum-python)\n",
      "  Downloading cudensitymat_cu12-0.3.1-py3-none-manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting cupy-cuda12x>=13.0 (from cuquantum-python-cu12==25.09.1->cuquantum-python)\n",
      "  Downloading cupy_cuda12x-13.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting cuda-bindings<13.0.0,>=12.9.2 (from cuquantum-python-cu12==25.09.1->cuquantum-python)\n",
      "  Downloading cuda_bindings-12.9.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting cuda-core<0.4,>=0.3.2 (from nvmath-python==0.6.0->cuquantum-python-cu12==25.09.1->cuquantum-python)\n",
      "  Downloading cuda_core-0.3.2-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting cuda-pathfinder<2.0,>=1.2.1 (from nvmath-python==0.6.0->cuquantum-python-cu12==25.09.1->cuquantum-python)\n",
      "  Downloading cuda_pathfinder-1.3.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting cutensor-cu12<3,>=2.3.1 (from cudensitymat-cu12<0.4,>=0.3.1->cuquantum-python-cu12==25.09.1->cuquantum-python)\n",
      "  Downloading cutensor_cu12-2.3.1-py3-none-manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x>=13.0->cuquantum-python-cu12==25.09.1->cuquantum-python)\n",
      "  Downloading fastrlock-0.8.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading cuquantum_python_cu12-25.9.1-74-cp311-cp311-manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvmath_python-0.6.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m151.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cuda_bindings-12.9.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m162.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cuda_core-0.3.2-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cuda_pathfinder-1.3.1-py3-none-any.whl (27 kB)\n",
      "Downloading cudensitymat_cu12-0.3.1-py3-none-manylinux2014_x86_64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m139.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cutensor_cu12-2.3.1-py3-none-manylinux2014_x86_64.whl (237.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.1/237.1 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cutensornet_cu12-2.9.1-py3-none-manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m126.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cupy_cuda12x-13.6.0-cp311-cp311-manylinux2014_x86_64.whl (113.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.0/113.0 MB\u001b[0m \u001b[31m186.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastrlock-0.8.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (54 kB)\n",
      "Building wheels for collected packages: cuquantum-python\n",
      "\u001b[33m  DEPRECATION: Building 'cuquantum-python' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'cuquantum-python'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for cuquantum-python (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cuquantum-python: filename=cuquantum_python-25.9.1-0_cu12-py3-none-any.whl size=6575 sha256=d7f4a4aa674cc05aac22646fa83d980a42b20a45a17046144fe02594223427b8\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a8/19/23/e75d628b09adde5e96955678cd259ef9e11b5afc7a23364295\n",
      "Successfully built cuquantum-python\n",
      "Installing collected packages: fastrlock, cutensor-cu12, cutensornet-cu12, cupy-cuda12x, cuda-pathfinder, cuda-core, cudensitymat-cu12, cuda-bindings, nvmath-python, cuquantum-python-cu12, cuquantum-python\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/11\u001b[0m [cuquantum-python][nvmath-python]u12]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cuda-bindings-12.9.4 cuda-core-0.3.2 cuda-pathfinder-1.3.1 cudensitymat-cu12-0.3.1 cupy-cuda12x-13.6.0 cuquantum-python-25.9.1 cuquantum-python-cu12-25.9.1 cutensor-cu12-2.3.1 cutensornet-cu12-2.9.1 fastrlock-0.8.3 nvmath-python-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"pennylane-lightning[gpu]\" --upgrade\n",
    "!pip install cuquantum-python --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9845ea5f-cb9d-4578-9560-bb5b8a9932fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.qbraid/environments/h_mmigxh/pyenv/lib/python3.11/site-packages/pennylane/__init__.py:209: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.6.2. You have version 0.8.0 installed. Please downgrade JAX to 0.6.2 to avoid runtime errors using python -m pip install jax~=0.6.0 jaxlib~=0.6.0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK <lightning.gpu device (wires=1) at 0x7cc82c1276d0>\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "dev = qml.device(\"lightning.gpu\", wires=1)\n",
    "print(\"OK\", dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73415e5b-7ae4-4aaa-b2fb-574721648aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jax\n",
      "  Using cached jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.8.0,>=0.8.0 (from jax)\n",
      "  Using cached jaxlib-0.8.0-cp311-cp311-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting ml_dtypes>=0.5.0 (from jax)\n",
      "  Using cached ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: numpy>=2.0 in /opt/conda/lib/python3.11/site-packages (from jax) (2.3.3)\n",
      "Collecting opt_einsum (from jax)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: scipy>=1.13 in /opt/conda/lib/python3.11/site-packages (from jax) (1.16.2)\n",
      "Using cached jax-0.8.0-py3-none-any.whl (2.9 MB)\n",
      "Using cached jaxlib-0.8.0-cp311-cp311-manylinux_2_27_x86_64.whl (79.7 MB)\n",
      "Using cached ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Installing collected packages: opt_einsum, ml_dtypes, jaxlib, jax\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [jax]\u001b[32m3/4\u001b[0m [jax]ib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jax-0.8.0 jaxlib-0.8.0 ml_dtypes-0.5.3 opt_einsum-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933769e3-482a-4c50-a6bc-291b2026153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载 lightning.gpu 设备，将使用 NVIDIA cuQuantum 加速。\n",
      "正在生成数据...\n",
      "样本总数：6046\n",
      "类别计数: {'draw': 4536, 'circle': 490, 'cross': 1020}\n",
      "使用的 Class Weights: {'draw': 0.44429747207524983, 'circle': 4.112925170068027, 'cross': 1.9758169934640524}\n",
      "训练集大小: 10887, 测试集大小: 1209\n",
      "训练集类别分布: Counter({np.int32(0): 3629, np.int32(1): 3629, np.int32(2): 3629})\n",
      "测试集类别分布: Counter({np.int32(1): 907, np.int32(2): 204, np.int32(0): 98})\n",
      "JIT 编译中（第一次调用会比较慢）...\n",
      "编译完成。开始训练。\n",
      "Epoch 1/15 - T=1573.52s - Loss=2.2151 - Test Acc=0.145\n",
      "Epoch 2/15 - Batch 2/85"
     ]
    }
   ],
   "source": [
    "# Jax\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax  # JAX 生态中的主流优化器库\n",
    "import itertools, time\n",
    "from collections import defaultdict, Counter\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ----------------------------\n",
    "# JAX \n",
    "# ----------------------------\n",
    "# JAX \n",
    "#  \"lightning.gpu\" or PennyLane NVIDIA  cuQuantum SDK\n",
    "\n",
    "WIRE_COUNT = 9\n",
    "try:\n",
    "    dev = qml.device(\"lightning.gpu\", wires=WIRE_COUNT)\n",
    "    print(\"成功加载 lightning.gpu 设备，将使用 NVIDIA cuQuantum 加速。\")\n",
    "except (qml.DeviceError, ImportError):\n",
    "    print(\"lightning.gpu 不可用，回退到 lightning.qubit。\")\n",
    "    dev = qml.device(\"lightning.qubit\", wires=WIRE_COUNT)\n",
    "\n",
    "# ----------------------------\n",
    "# model\n",
    "# ----------------------------\n",
    "#  Python list or np.array not jnp.array\n",
    "corners = [0, 2, 6, 8]\n",
    "edges = [1, 3, 5, 7]\n",
    "center = 4\n",
    "\n",
    "# increase the model size\n",
    "\n",
    "n_layers = 2\n",
    "p_repetitions = 2\n",
    "params_per_block = 6\n",
    "total_params = (n_layers + 1) * p_repetitions * params_per_block\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def is_valid_game(board):\n",
    "    x_count = np.sum(board == 1)\n",
    "    o_count = np.sum(board == -1)\n",
    "    return (x_count == o_count) or (x_count == o_count + 1)\n",
    "\n",
    "def check_winner_terminal(board):\n",
    "    lines = [\n",
    "        [0,1,2],[3,4,5],[6,7,8],\n",
    "        [0,3,6],[1,4,7],[2,5,8],\n",
    "        [0,4,8],[2,4,6]\n",
    "    ]\n",
    "    board_tuple = tuple(board)\n",
    "    for line in lines:\n",
    "        if board_tuple[line[0]] == board_tuple[line[1]] == board_tuple[line[2]] == 1:\n",
    "            return 'cross'\n",
    "        if board_tuple[line[0]] == board_tuple[line[1]] == board_tuple[line[2]] == -1:\n",
    "            return 'circle'\n",
    "    return 'draw'\n",
    "\n",
    "def generate_all_valid_games(subset_size=None):\n",
    "    games = []\n",
    "    # increase size\n",
    "    for num_pieces in range(10): \n",
    "        for positions in itertools.combinations(range(9), num_pieces):\n",
    "            num_x = (num_pieces + 1) // 2\n",
    "            for x_pos in itertools.combinations(positions, num_x):\n",
    "                board = np.zeros(9, dtype=np.int8)\n",
    "                o_pos = list(set(positions) - set(x_pos))\n",
    "                board[list(x_pos)] = 1\n",
    "                board[o_pos] = -1\n",
    "                label = check_winner_terminal(board)\n",
    "                games.append((board, label))\n",
    "    \n",
    "    unique_games_set = set([(tuple(b), l) for b, l in games])\n",
    "    games = [(np.array(b), l) for b, l in unique_games_set]\n",
    "    \n",
    "    if subset_size is not None and subset_size < len(games):\n",
    "        idx = rng.choice(len(games), size=subset_size, replace=False)\n",
    "        games = [games[i] for i in idx]\n",
    "        \n",
    "    rng.shuffle(games)\n",
    "    return games\n",
    "\n",
    "\n",
    "def data_encoding_pennylane(board):\n",
    "    for i, v in enumerate(board):\n",
    "        qml.RZ(v * jnp.pi, wires=i)\n",
    "        qml.RY(v * (jnp.pi/2), wires=i)\n",
    "\n",
    "def branch_block_center(params):\n",
    "    _, _, th_m, th_ce, th_ec, _ = params\n",
    "    qml.RY(th_m, wires=center)\n",
    "    for c in corners:\n",
    "        qml.CRY(th_ce/6, wires=[center, c])\n",
    "    for e in edges:\n",
    "        qml.CRY(th_ec/6, wires=[center, e])\n",
    "\n",
    "def branch_block_corners(params):\n",
    "    th_c, _, _, _, _, th_cm = params\n",
    "    for q in corners:\n",
    "        qml.RY(th_c, wires=q)\n",
    "    pairs = [(0,2),(2,8),(8,6),(6,0)]\n",
    "    for a,b in pairs:\n",
    "        qml.CZ(wires=[a,b])\n",
    "    for c in corners:\n",
    "        qml.CRY(th_cm/2, wires=[center, c])\n",
    "\n",
    "def branch_block_edges(params):\n",
    "    _, th_e, _, _, th_ec, _ = params\n",
    "    for q in edges:\n",
    "        qml.RY(th_e, wires=q)\n",
    "    pairs = [(1,3),(3,7),(7,5),(5,1)]\n",
    "    for a,b in pairs:\n",
    "        qml.CZ(wires=[a,b])\n",
    "    for e in edges:\n",
    "        qml.CRY(th_ec/2, wires=[center, e])\n",
    "\n",
    "def hierarchical_control_example(control_strength=0.1):\n",
    "    for c in corners:\n",
    "        qml.CRY(control_strength, wires=[center, c])\n",
    "    for e in edges:\n",
    "        qml.CRY(control_strength, wires=[center, e])\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface='jax')\n",
    "def qnode_tree(board, params):\n",
    "    data_encoding_pennylane(board)\n",
    "    param_idx = 0\n",
    "    for _ in range(p_repetitions):\n",
    "        block_params = jax.lax.dynamic_slice_in_dim(params, param_idx, 6, axis=0)\n",
    "        branch_block_center(block_params)\n",
    "        hierarchical_control_example(control_strength=0.05)\n",
    "        branch_block_corners(block_params)\n",
    "        branch_block_edges(block_params)\n",
    "        param_idx += 6\n",
    "\n",
    "    for _ in range(n_layers):\n",
    "        data_encoding_pennylane(board)\n",
    "        for _ in range(p_repetitions):\n",
    "            block_params = jax.lax.dynamic_slice_in_dim(params, param_idx, 6, axis=0)\n",
    "            branch_block_center(block_params)\n",
    "            hierarchical_control_example(control_strength=block_params[5]*0.05)\n",
    "            branch_block_corners(block_params)\n",
    "            branch_block_edges(block_params)\n",
    "            param_idx += 6\n",
    "    \n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(WIRE_COUNT)]\n",
    "\n",
    "\n",
    "label_to_idx = {'circle':0, 'draw':1, 'cross':2}\n",
    "idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
    "\n",
    "def to_logits_from_qnode_output(qnode_out):\n",
    "    corner_expvals = qnode_out[jnp.array(corners)]\n",
    "    edge_expvals = qnode_out[jnp.array(edges)]\n",
    "    \n",
    "    corner_avg = jnp.mean(corner_expvals)\n",
    "    center_val = qnode_out[center]\n",
    "    edge_avg = jnp.mean(edge_expvals)\n",
    "    \n",
    "    return jnp.stack([corner_avg, center_val, edge_avg])\n",
    "\n",
    "batched_qnode = jax.vmap(qnode_tree, in_axes=(0, None))\n",
    "batched_logits_computer = jax.vmap(to_logits_from_qnode_output, in_axes=(0))\n",
    "\n",
    "def compute_logits(boards, params):\n",
    "    qnode_outputs = jnp.array(batched_qnode(boards, params)).T\n",
    "    return batched_logits_computer(qnode_outputs)\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params, boards, labels, class_weights_arr):\n",
    "    logits = compute_logits(boards, params)\n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes=3)\n",
    "    weights = jnp.sum(one_hot_labels * class_weights_arr, axis=1)\n",
    "    unweighted_loss = optax.softmax_cross_entropy(logits, one_hot_labels)\n",
    "    weighted_loss = unweighted_loss * weights\n",
    "    return jnp.mean(weighted_loss)\n",
    "\n",
    "@jax.jit\n",
    "def accuracy_fn(params, boards, labels):\n",
    "    logits = compute_logits(boards, params)\n",
    "    predictions = jnp.argmax(logits, axis=1)\n",
    "    return jnp.mean(predictions == labels)\n",
    "\n",
    "\n",
    "print(\"Generating...\")\n",
    "all_games = generate_all_valid_games()\n",
    "print(f\"Total：{len(all_games)}\")\n",
    "\n",
    "games_by_class = defaultdict(list)\n",
    "for b, l in all_games:\n",
    "    games_by_class[l].append(b)\n",
    "\n",
    "counts = {lab: len(games_by_class[lab]) for lab in games_by_class}\n",
    "total = sum(counts.values())\n",
    "class_weights_map = {lab: total / (len(counts) * count) for lab, count in counts.items() if count > 0}\n",
    "print(\"Counts:\", counts)\n",
    "print(\"Used Class Weights:\", class_weights_map)\n",
    "\n",
    "class_weights_arr = jnp.array([\n",
    "    class_weights_map.get('circle', 1.0),\n",
    "    class_weights_map.get('draw', 1.0),\n",
    "    class_weights_map.get('cross', 1.0)\n",
    "])\n",
    "\n",
    "test_boards, test_labels, train_boards, train_labels = [], [], [], []\n",
    "for lab, idx in label_to_idx.items():\n",
    "    boards = games_by_class.get(lab, [])\n",
    "    if not boards: continue\n",
    "    rng.shuffle(boards)\n",
    "    \n",
    "    test_split_idx = len(boards) // 5\n",
    "    test_boards.extend(boards[:test_split_idx])\n",
    "    test_labels.extend([idx] * len(boards[:test_split_idx]))\n",
    "    \n",
    "    train_boards.extend(boards[test_split_idx:])\n",
    "    train_labels.extend([idx] * len(boards[test_split_idx:]))\n",
    "\n",
    "if train_labels:\n",
    "    class_counts = Counter(train_labels)\n",
    "    max_class_size = max(class_counts.values()) if class_counts else 0\n",
    "    balanced_train_boards, balanced_train_labels = [], []\n",
    "    for lab, idx in label_to_idx.items():\n",
    "        class_boards = [b for b, l in zip(train_boards, train_labels) if l == idx]\n",
    "        if not class_boards: continue\n",
    "        \n",
    "        repeats = (max_class_size + len(class_boards) - 1) // len(class_boards)\n",
    "        resampled = (class_boards * repeats)[:max_class_size]\n",
    "        \n",
    "        balanced_train_boards.extend(resampled)\n",
    "        balanced_train_labels.extend([idx] * max_class_size)\n",
    "\n",
    "    train_boards = jnp.array(balanced_train_boards)\n",
    "    train_labels = jnp.array(balanced_train_labels)\n",
    "else:\n",
    "    train_boards, train_labels = jnp.array([]), jnp.array([])\n",
    "\n",
    "test_boards = jnp.array(test_boards)\n",
    "test_labels = jnp.array(test_labels)\n",
    "\n",
    "print(f\"Train size: {len(train_boards)}, Validation size: {len(test_boards)}\")\n",
    "print(f\"Train distribution: {Counter(np.array(train_labels))}\")\n",
    "print(f\"Validation distribution : {Counter(np.array(test_labels))}\")\n",
    "# ----------------------------\n",
    "# 训练循环\n",
    "# ----------------------------\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = jax.random.uniform(key, (total_params,), minval=-0.05, maxval=0.05)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=0.01)\n",
    "opt_state = optimizer.init(params)\n",
    "grad_fn = jax.value_and_grad(loss_fn)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "if len(train_boards) > 0 and len(test_boards) > 0:\n",
    "    print(\"JIT compliing...\")\n",
    "    dummy_boards = train_boards[:2]\n",
    "    dummy_labels = train_labels[:2]\n",
    "    _ = loss_fn(params, dummy_boards, dummy_labels, class_weights_arr)\n",
    "    _ = accuracy_fn(params, dummy_boards, dummy_labels)\n",
    "    print(\"complied。Start training。\")\n",
    "    \n",
    "    start_all = time.time()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t_epoch = time.time()\n",
    "        \n",
    "        perms = jax.random.permutation(key, len(train_boards))\n",
    "        key, _ = jax.random.split(key)\n",
    "        \n",
    "        batch_losses = []\n",
    "        num_batches = len(train_boards) // batch_size\n",
    "        \n",
    "        \n",
    "        for i in range(0, len(train_boards), batch_size):\n",
    "           \n",
    "            batch_num = i // batch_size + 1\n",
    "            sys.stdout.write(f\"\\rEpoch {epoch}/{epochs} - Batch {batch_num}/{num_batches}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            batch_idx = perms[i:i+batch_size]\n",
    "            batch_boards = train_boards[batch_idx]\n",
    "            batch_labels = train_labels[batch_idx]\n",
    "            \n",
    "            loss_val, grads = grad_fn(params, batch_boards, batch_labels, class_weights_arr)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "            batch_losses.append(loss_val)\n",
    "    \n",
    "        epoch_time = time.time() - t_epoch\n",
    "        avg_loss = jnp.mean(jnp.array(batch_losses))\n",
    "        test_acc = accuracy_fn(params, test_boards, test_labels)\n",
    "        \n",
    "        \n",
    "        sys.stdout.write(\"\\r\" + \" \" * 50 + \"\\r\") \n",
    "        sys.stdout.flush()\n",
    "        print(f\"Epoch {epoch}/{epochs} - T={epoch_time:.2f}s - Loss={avg_loss:.4f} - Test Acc={test_acc:.3f}\")\n",
    "    \n",
    "    total_time = time.time() - start_all\n",
    "    print(f\"Time needed：{total_time:.1f}s\")\n",
    "    \n",
    "    # 最终评估\n",
    "    final_train_acc = accuracy_fn(params, train_boards, train_labels)\n",
    "    final_test_acc = accuracy_fn(params, test_boards, test_labels)\n",
    "    print(f\"Final Train Acc: {final_train_acc:.4f}\")\n",
    "    print(f\"Final Test  Acc: {final_test_acc:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Not enough data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eac888-f811-4655-8501-79e0377e5235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [h]",
   "language": "python",
   "name": "python3_h_mmigxh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
